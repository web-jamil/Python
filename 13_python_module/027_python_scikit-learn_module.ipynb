{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Python `scikit-learn` Module: Concepts and Theory**\n",
    "\n",
    "`scikit-learn` is one of the most popular Python libraries for machine learning and data analysis. It provides a wide array of tools for data preprocessing, model fitting, evaluation, and model selection. Built on top of `NumPy`, `SciPy`, and `matplotlib`, it is a powerful and efficient library for building machine learning models. `scikit-learn` supports both supervised and unsupervised learning algorithms, as well as tools for data preprocessing, feature selection, and model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts of `scikit-learn`**\n",
    "\n",
    "1. **Supervised vs. Unsupervised Learning**:\n",
    "\n",
    "   - **Supervised Learning**: The model learns from labeled data (input-output pairs) to make predictions on unseen data. Examples: Linear Regression, Classification algorithms.\n",
    "   - **Unsupervised Learning**: The model learns from unlabeled data to identify patterns or structures (e.g., clustering). Examples: K-means, PCA.\n",
    "\n",
    "2. **Estimator**:\n",
    "\n",
    "   - In `scikit-learn`, an estimator is any object that learns from data and can make predictions. Estimators can be divided into two categories:\n",
    "     - **Supervised Learning Estimators**: These are used for tasks where the data has known outputs (labels), such as classification or regression. Examples: `LogisticRegression`, `RandomForestClassifier`.\n",
    "     - **Unsupervised Learning Estimators**: These are used for tasks where the data doesn't have labels, like clustering or dimensionality reduction. Examples: `KMeans`, `PCA`.\n",
    "\n",
    "3. **Pipeline**:\n",
    "\n",
    "   - A `Pipeline` is a way of bundling together several steps in a machine learning process, such as preprocessing and model fitting, into a single object. This allows for a cleaner and more organized workflow, especially in the case of cross-validation and grid search for hyperparameter tuning.\n",
    "   - Example:\n",
    "\n",
    "     ```python\n",
    "     from sklearn.pipeline import Pipeline\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "     from sklearn.svm import SVC\n",
    "\n",
    "     pipeline = Pipeline([\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('svc', SVC())\n",
    "     ])\n",
    "     ```\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "\n",
    "   - `scikit-learn` provides various tools to evaluate the performance of a machine learning model, including metrics for regression and classification tasks. Common evaluation metrics include accuracy, precision, recall, F1-score, ROC AUC, mean squared error, etc.\n",
    "   - Example:\n",
    "\n",
    "     ```python\n",
    "     from sklearn.metrics import accuracy_score\n",
    "\n",
    "     y_pred = model.predict(X_test)\n",
    "     accuracy = accuracy_score(y_test, y_pred)\n",
    "     ```\n",
    "\n",
    "5. **Cross-Validation**:\n",
    "   - Cross-validation is a technique for assessing how the results of a statistical analysis will generalize to an independent dataset. `scikit-learn` provides `cross_val_score()` to help you evaluate a model's performance on multiple subsets of data, reducing overfitting.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.model_selection import cross_val_score\n",
    "     cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Modules and Functions in `scikit-learn`**\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "\n",
    "   - **Scaling and Normalization**: It's crucial to scale or normalize the data when working with many machine learning algorithms, as they may be sensitive to the scale of the features.\n",
    "     - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\n",
    "     - `MinMaxScaler`: Scales features to a given range, typically [0, 1].\n",
    "     - `RobustScaler`: Scales using statistics that are robust to outliers.\n",
    "\n",
    "   Example:\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "   scaler = StandardScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   ```\n",
    "\n",
    "2. **Feature Selection and Engineering**:\n",
    "\n",
    "   - `scikit-learn` offers a range of tools to perform feature selection and dimensionality reduction, which can improve model performance and reduce overfitting.\n",
    "     - **Feature selection**: You can select the most important features using `SelectKBest`, `RFE`, or feature importance.\n",
    "     - **Dimensionality reduction**: You can reduce the number of features using techniques like PCA (Principal Component Analysis) or LDA (Linear Discriminant Analysis).\n",
    "\n",
    "   Example (PCA):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.decomposition import PCA\n",
    "   pca = PCA(n_components=2)\n",
    "   X_pca = pca.fit_transform(X)\n",
    "   ```\n",
    "\n",
    "3. **Supervised Learning Algorithms**:\n",
    "\n",
    "   - **Regression**: Predict continuous values based on input features.\n",
    "     - Linear Regression (`LinearRegression`)\n",
    "     - Decision Trees (`DecisionTreeRegressor`)\n",
    "     - Random Forest (`RandomForestRegressor`)\n",
    "     - Support Vector Regression (`SVR`)\n",
    "\n",
    "   Example (Linear Regression):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.linear_model import LinearRegression\n",
    "   model = LinearRegression()\n",
    "   model.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "   - **Classification**: Predict categorical values (classes) based on input features.\n",
    "     - Logistic Regression (`LogisticRegression`)\n",
    "     - Decision Trees (`DecisionTreeClassifier`)\n",
    "     - Random Forest (`RandomForestClassifier`)\n",
    "     - Support Vector Machines (`SVC`)\n",
    "\n",
    "   Example (Logistic Regression):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   model = LogisticRegression()\n",
    "   model.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "4. **Unsupervised Learning Algorithms**:\n",
    "\n",
    "   - **Clustering**: Group data into clusters.\n",
    "     - K-means Clustering (`KMeans`)\n",
    "     - Hierarchical Clustering (`AgglomerativeClustering`)\n",
    "     - DBSCAN (`DBSCAN`)\n",
    "\n",
    "   Example (K-means):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.cluster import KMeans\n",
    "   model = KMeans(n_clusters=3)\n",
    "   model.fit(X)\n",
    "   ```\n",
    "\n",
    "   - **Dimensionality Reduction**: Reduce the number of features while retaining as much information as possible.\n",
    "     - PCA (`PCA`)\n",
    "     - t-SNE (`TSNE`)\n",
    "     - LDA (`LinearDiscriminantAnalysis`)\n",
    "\n",
    "5. **Model Selection and Hyperparameter Tuning**:\n",
    "\n",
    "   - **Grid Search**: Use `GridSearchCV` to find the best combination of hyperparameters for a model. It exhaustively searches a specified parameter grid.\n",
    "\n",
    "   Example (Grid Search):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import GridSearchCV\n",
    "   param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "   grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "   grid_search.fit(X_train, y_train)\n",
    "   best_params = grid_search.best_params_\n",
    "   ```\n",
    "\n",
    "   - **Random Search**: Use `RandomizedSearchCV` to randomly sample hyperparameters, which is computationally more efficient than grid search.\n",
    "\n",
    "   Example (Random Search):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import RandomizedSearchCV\n",
    "   param_dist = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "   random_search = RandomizedSearchCV(SVC(), param_dist, cv=5)\n",
    "   random_search.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "6. **Ensemble Methods**:\n",
    "\n",
    "   - Ensemble methods combine multiple models to produce a stronger model. Popular ensemble techniques include:\n",
    "     - **Bagging**: Training multiple models on random subsets of the data and averaging the results. Example: Random Forest.\n",
    "     - **Boosting**: Training multiple models sequentially, where each model attempts to correct the errors of the previous model. Example: Gradient Boosting, AdaBoost.\n",
    "     - **Stacking**: Combining the predictions of several models via another model (meta-model).\n",
    "\n",
    "   Example (Random Forest):\n",
    "\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   model = RandomForestClassifier(n_estimators=100)\n",
    "   model.fit(X_train, y_train)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Evaluation and Metrics**\n",
    "\n",
    "`scikit-learn` provides several tools to evaluate the performance of a model. Some common metrics are:\n",
    "\n",
    "1. **Accuracy**:\n",
    "\n",
    "   - Used for classification models to evaluate the percentage of correct predictions.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import accuracy_score\n",
    "   accuracy = accuracy_score(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "2. **Precision, Recall, F1-Score**:\n",
    "\n",
    "   - Precision: The proportion of true positive predictions among all positive predictions.\n",
    "   - Recall: The proportion of true positive predictions among all actual positive instances.\n",
    "   - F1-Score: The harmonic mean of precision and recall.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import classification_report\n",
    "   print(classification_report(y_true, y_pred))\n",
    "   ```\n",
    "\n",
    "3. **Confusion Matrix**:\n",
    "\n",
    "   - A matrix showing the actual vs predicted values, often used for evaluating classification models.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import confusion_matrix\n",
    "   confusion_matrix(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "4. **Mean Squared Error (MSE)**:\n",
    "\n",
    "   - A common metric for regression models, measuring the average squared difference between predicted and actual values.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.metrics import mean_squared_error\n",
    "   mse = mean_squared_error(y_true, y_pred)\n",
    "   ```\n",
    "\n",
    "5. **Cross-Validation**:\n",
    "\n",
    "   - `cross_val_score` is used for performing k-fold cross-validation to assess the model’s performance across different data splits.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.model_selection import cross_val_score\n",
    "   scores = cross_val_score(model, X, y, cv=5)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "`scikit-learn` is a comprehensive, user-friendly, and efficient machine learning library in Python. It provides a range of algorithms and tools for preprocessing, training, and evaluating machine learning models. With support for both supervised and unsupervised learning, hyperparameter tuning, and model evaluation, `scikit-learn` is widely used in data science and machine learning applications.\n",
    "\n",
    "By combining `scikit-learn` with other libraries like `pandas`, `matplotlib`, and `seaborn`, you can build sophisticated machine learning pipelines for data analysis and predictive modeling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
