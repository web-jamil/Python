{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Python `TensorFlow` Module: Concepts and Theory**\n",
    "\n",
    "`TensorFlow` is an open-source machine learning framework developed by Google for building and deploying machine learning models. Initially developed for deep learning applications, it has expanded to include tools for a wide range of machine learning tasks, including supervised, unsupervised, reinforcement learning, and other neural network-based approaches. `TensorFlow` provides a high-level API called `Keras` for building and training models and also offers lower-level control for more complex machine learning workflows.\n",
    "\n",
    "Hereâ€™s a breakdown of the essential concepts and components of `TensorFlow`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Concepts of TensorFlow**\n",
    "\n",
    "1. **Tensors**:\n",
    "\n",
    "   - Tensors are the fundamental building blocks of `TensorFlow`. A tensor is simply a multi-dimensional array or matrix, similar to arrays in `NumPy`. The term \"tensor\" comes from the fact that the array can have more than two dimensions.\n",
    "   - Common tensor shapes include scalars (0D), vectors (1D), matrices (2D), and higher-dimensional tensors (3D, 4D, etc.).\n",
    "\n",
    "   Example of creating tensors in TensorFlow:\n",
    "\n",
    "   ```python\n",
    "   import tensorflow as tf\n",
    "\n",
    "   scalar = tf.constant(5)  # 0D tensor\n",
    "   vector = tf.constant([1, 2, 3])  # 1D tensor\n",
    "   matrix = tf.constant([[1, 2], [3, 4]])  # 2D tensor\n",
    "   ```\n",
    "\n",
    "2. **Operations on Tensors**:\n",
    "\n",
    "   - `TensorFlow` supports a wide variety of operations on tensors, such as element-wise arithmetic, matrix operations, reductions (like summing or finding the mean), and reshaping tensors.\n",
    "\n",
    "   Example:\n",
    "\n",
    "   ```python\n",
    "   tensor1 = tf.constant([1, 2, 3])\n",
    "   tensor2 = tf.constant([4, 5, 6])\n",
    "   result = tf.add(tensor1, tensor2)  # Element-wise addition\n",
    "   ```\n",
    "\n",
    "3. **Computational Graph**:\n",
    "\n",
    "   - TensorFlow models are based on a computational graph. The graph represents computations as nodes, and edges represent the data (tensors) flowing between nodes.\n",
    "   - TensorFlow 2.x simplifies the execution model with **eager execution**, which evaluates operations immediately. In contrast, TensorFlow 1.x required the graph to be explicitly created and executed in a session.\n",
    "\n",
    "4. **Sessions (TensorFlow 1.x)**:\n",
    "\n",
    "   - In TensorFlow 1.x, you define the graph and then launch a session to run the operations. However, with TensorFlow 2.x, eager execution is enabled by default, removing the need for explicit sessions.\n",
    "\n",
    "   Example in TensorFlow 1.x:\n",
    "\n",
    "   ```python\n",
    "   with tf.Session() as sess:\n",
    "       result = sess.run(tensor1 + tensor2)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Components of TensorFlow**\n",
    "\n",
    "1. **Keras API**:\n",
    "\n",
    "   - `Keras` is a high-level API for building and training neural networks in `TensorFlow`. It provides simple, user-friendly methods for defining, training, and evaluating models, and is now integrated into TensorFlow as the `tf.keras` module.\n",
    "   - `tf.keras` supports the construction of Sequential models and Functional API models, enabling flexible model architectures.\n",
    "\n",
    "   Example of a Sequential model in `tf.keras`:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.models import Sequential\n",
    "   from tensorflow.keras.layers import Dense\n",
    "\n",
    "   model = Sequential([\n",
    "       Dense(64, activation='relu', input_shape=(784,)),\n",
    "       Dense(10, activation='softmax')\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "2. **Layers**:\n",
    "\n",
    "   - In `TensorFlow`, a model is typically built by stacking layers. Layers are the building blocks of neural networks. Examples include:\n",
    "     - `Dense` (fully connected layer)\n",
    "     - `Conv2D` (convolutional layer)\n",
    "     - `LSTM` (long short-term memory layer)\n",
    "     - `Flatten` (flattening the output of a previous layer)\n",
    "\n",
    "   Example of a simple neural network layer:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.layers import Dense\n",
    "\n",
    "   layer = Dense(64, activation='relu')  # Fully connected layer with ReLU activation\n",
    "   ```\n",
    "\n",
    "3. **Optimizers**:\n",
    "\n",
    "   - An optimizer in `TensorFlow` is used to minimize (or maximize) the loss function during model training by adjusting the model's weights. Popular optimizers include:\n",
    "     - `SGD` (Stochastic Gradient Descent)\n",
    "     - `Adam` (Adaptive Moment Estimation)\n",
    "     - `RMSProp`\n",
    "\n",
    "   Example of using the `Adam` optimizer:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.optimizers import Adam\n",
    "   optimizer = Adam(learning_rate=0.001)\n",
    "   ```\n",
    "\n",
    "4. **Loss Functions**:\n",
    "\n",
    "   - A loss function is used to measure the discrepancy between the predicted output and the actual output (ground truth). For regression tasks, `Mean Squared Error` (MSE) is commonly used. For classification tasks, `Categorical Crossentropy` or `Binary Crossentropy` are used.\n",
    "\n",
    "   Example of using a loss function in Keras:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "   loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "   ```\n",
    "\n",
    "5. **Metrics**:\n",
    "\n",
    "   - Metrics are used to evaluate the performance of a model during training and evaluation. Common metrics include accuracy, precision, recall, and F1 score.\n",
    "\n",
    "   Example:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.metrics import Accuracy\n",
    "   metric = Accuracy()\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Deep Learning Architectures Supported by TensorFlow**\n",
    "\n",
    "1. **Feedforward Neural Networks (FNN)**:\n",
    "\n",
    "   - Also known as Multilayer Perceptrons (MLP), these are the most basic type of neural networks used for tasks like classification and regression.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNN)**:\n",
    "\n",
    "   - CNNs are primarily used for image processing tasks. They use convolutional layers to automatically learn spatial hierarchies of features from the input data (images, for example).\n",
    "\n",
    "   Example of a CNN architecture:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "   model = Sequential([\n",
    "       Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "       MaxPooling2D((2, 2)),\n",
    "       Flatten(),\n",
    "       Dense(10, activation='softmax')\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "3. **Recurrent Neural Networks (RNN)**:\n",
    "\n",
    "   - RNNs are used for sequential data processing, such as natural language processing (NLP) or time-series analysis. They have loops in their architecture that allow information to persist across time steps.\n",
    "\n",
    "   Example of an RNN model:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "   model = Sequential([\n",
    "       SimpleRNN(64, input_shape=(None, 10)),\n",
    "       Dense(1)\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "4. **Long Short-Term Memory (LSTM)**:\n",
    "\n",
    "   - LSTM is a type of RNN designed to overcome the vanishing gradient problem and retain long-term dependencies, making it suitable for tasks like speech recognition or machine translation.\n",
    "\n",
    "   Example of using LSTM:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.layers import LSTM\n",
    "\n",
    "   model = Sequential([\n",
    "       LSTM(64, input_shape=(None, 10)),\n",
    "       Dense(1)\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "5. **Transformer Networks**:\n",
    "\n",
    "   - The transformer architecture is widely used in natural language processing tasks (e.g., machine translation, text generation). It relies on the self-attention mechanism to process input sequences in parallel, which is more efficient than RNNs or LSTMs.\n",
    "\n",
    "   Example of using transformers with TensorFlow is beyond basic examples, but `TensorFlow` provides `tf.keras.layers.MultiHeadAttention` to implement such architectures.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training and Evaluation in TensorFlow**\n",
    "\n",
    "1. **Model Training**:\n",
    "\n",
    "   - `TensorFlow` offers a method `fit()` to train a model. During training, the model weights are updated by optimizing the loss function using an optimizer.\n",
    "\n",
    "   Example of training:\n",
    "\n",
    "   ```python\n",
    "   model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "   model.fit(X_train, y_train, epochs=10)\n",
    "   ```\n",
    "\n",
    "2. **Validation and Testing**:\n",
    "\n",
    "   - After training the model, it is important to evaluate its performance on unseen data (validation or test data).\n",
    "\n",
    "   Example:\n",
    "\n",
    "   ```python\n",
    "   model.evaluate(X_test, y_test)\n",
    "   ```\n",
    "\n",
    "3. **Model Saving and Loading**:\n",
    "\n",
    "   - Once a model is trained, it can be saved to disk for later use. `TensorFlow` allows saving models as `.h5` (HDF5) files or as TensorFlow's native `SavedModel` format.\n",
    "\n",
    "   Saving a model:\n",
    "\n",
    "   ```python\n",
    "   model.save('my_model.h5')\n",
    "   ```\n",
    "\n",
    "   Loading a model:\n",
    "\n",
    "   ```python\n",
    "   from tensorflow.keras.models import load_model\n",
    "   model = load_model('my_model.h5')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **TensorFlow Ecosystem**\n",
    "\n",
    "1. **TensorFlow Lite**:\n",
    "\n",
    "   - A version of TensorFlow designed for mobile and embedded devices. It allows for optimized inference on mobile platforms with limited resources.\n",
    "\n",
    "2. **TensorFlow Serving**:\n",
    "\n",
    "   - A tool for deploying machine learning models in production environments, optimized for high-performance inference.\n",
    "\n",
    "3. **TensorFlow.js**:\n",
    "\n",
    "   - A JavaScript library for training and deploying machine learning models in the browser and on Node.js.\n",
    "\n",
    "4. **TensorFlow Extended (TFX)**:\n",
    "   - A set of tools to help deploy machine learning pipelines in production environments, enabling end-to-end ML workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "`TensorFlow` is a powerful and flexible deep learning framework with tools that are suitable for building and deploying machine learning models across a wide range of domains. It supports all types of machine learning algorithms, from basic linear regression to complex deep learning architectures like CNNs and LSTMs. By offering high-level APIs like `tf.keras`, TensorFlow makes building models easier and more accessible for both beginners and experts in the field.\n",
    "\n",
    "The `TensorFlow` ecosystem also provides tools for model deployment, mobile integration, and real-time inference, making it an excellent choice for production-ready machine learning solutions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
