{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spacy Module: Comprehensive Guide**\n",
    "\n",
    "**SpaCy** is an open-source library for Natural Language Processing (NLP) in Python. It is designed to be fast, efficient, and easy to use, making it a popular choice for tasks such as text processing, named entity recognition (NER), part-of-speech tagging (POS), dependency parsing, and more.\n",
    "\n",
    "This guide provides a comprehensive overview of the **spaCy** module, covering basic to advanced concepts and functionalities for text processing in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction to spaCy](#introduction-to-spacy)\n",
    "2. [Installation](#installation)\n",
    "3. [Basic Concepts](#basic-concepts)\n",
    "   - Tokenization\n",
    "   - Part-of-Speech (POS) Tagging\n",
    "   - Named Entity Recognition (NER)\n",
    "   - Dependency Parsing\n",
    "4. [Working with spaCy Models](#working-with-spacy-models)\n",
    "5. [Text Processing and Linguistic Features](#text-processing-and-linguistic-features)\n",
    "6. [Advanced Features](#advanced-features)\n",
    "   - Text Classification\n",
    "   - Word Vectors and Similarity\n",
    "   - Custom Pipelines and Components\n",
    "7. [Training Custom Models](#training-custom-models)\n",
    "8. [Applications of spaCy](#applications-of-spacy)\n",
    "9. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to spaCy**\n",
    "\n",
    "**spaCy** is an industrial-strength NLP library built specifically for fast processing and production pipelines. It is designed for real-world use cases, including large-scale NLP tasks. The library focuses on performance and ease of use.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Fast and Efficient**: spaCy is optimized for performance and can process text quickly.\n",
    "- **Pre-trained Models**: spaCy includes pre-trained models for multiple languages, including English, German, French, and Spanish.\n",
    "- **Deep Learning Integration**: It integrates seamlessly with deep learning frameworks like TensorFlow, PyTorch, and others.\n",
    "- **Preprocessing and Feature Extraction**: spaCy includes tokenization, POS tagging, named entity recognition, and syntactic analysis out of the box.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Installation**\n",
    "\n",
    "To install spaCy, you can use `pip` or `conda`. To get started, first install spaCy and download a model.\n",
    "\n",
    "### Install spaCy via `pip`:\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "### Download a Pre-trained Model:\n",
    "\n",
    "After installing spaCy, you need to download a model. For example, to download the English model `en_core_web_sm`:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "- `en_core_web_sm` is a small English model for common NLP tasks.\n",
    "- You can also choose larger models like `en_core_web_md` (medium) or `en_core_web_lg` (large).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Basic Concepts**\n",
    "\n",
    "### **3.1 Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting text into individual words or tokens. In spaCy, tokenization is handled by the `Doc` object, which contains a sequence of tokens.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"SpaCy is an amazing library for NLP!\")\n",
    "\n",
    "# Tokenization\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SpaCy\n",
    "is\n",
    "an\n",
    "amazing\n",
    "library\n",
    "for\n",
    "NLP\n",
    "!\n",
    "```\n",
    "\n",
    "### **3.2 Part-of-Speech (POS) Tagging**\n",
    "\n",
    "POS tagging is the process of determining the grammatical category of each token (e.g., noun, verb, adjective). spaCy tags tokens with POS tags using the built-in model.\n",
    "\n",
    "```python\n",
    "# POS tagging\n",
    "for token in doc:\n",
    "    print(f'{token.text}: {token.pos_}')\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SpaCy: PROPN\n",
    "is: AUX\n",
    "an: DET\n",
    "amazing: ADJ\n",
    "library: NOUN\n",
    "for: ADP\n",
    "NLP: PROPN\n",
    "!: PUNCT\n",
    "```\n",
    "\n",
    "### **3.3 Named Entity Recognition (NER)**\n",
    "\n",
    "NER is the task of identifying and classifying named entities (e.g., person names, organizations, dates, etc.). spaCy uses its pre-trained models to automatically extract entities from text.\n",
    "\n",
    "```python\n",
    "# Named Entity Recognition\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.text} - {ent.label_}')\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SpaCy - ORG\n",
    "NLP - ORG\n",
    "```\n",
    "\n",
    "### **3.4 Dependency Parsing**\n",
    "\n",
    "Dependency parsing analyzes the grammatical structure of a sentence, establishing relationships between words. In spaCy, this is done through the `dep_` attribute of tokens.\n",
    "\n",
    "```python\n",
    "# Dependency Parsing\n",
    "for token in doc:\n",
    "    print(f'{token.text}: {token.dep_} - {token.head.text}')\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SpaCy: nsubj - is\n",
    "is: ROOT - is\n",
    "an: det - library\n",
    "amazing: amod - library\n",
    "library: attr - is\n",
    "for: prep - library\n",
    "NLP: pobj - for\n",
    "!: punct - is\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Working with spaCy Models**\n",
    "\n",
    "spaCy provides multiple pre-trained models for various languages. These models contain information about tokenization, POS tagging, NER, and other linguistic features.\n",
    "\n",
    "### **Loading a Pre-trained Model**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"SpaCy is an amazing library for NLP!\")\n",
    "\n",
    "# Display the document\n",
    "print(doc)\n",
    "```\n",
    "\n",
    "### **Accessing Doc Attributes**\n",
    "\n",
    "Once the text is processed by the model, spaCy provides access to the following attributes:\n",
    "\n",
    "- `doc.text`: The full text of the document.\n",
    "- `doc.ents`: Named entities in the document.\n",
    "- `doc.sents`: Sentences in the document.\n",
    "- `doc.vector`: Word vector representation of the document.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Text Processing and Linguistic Features**\n",
    "\n",
    "### **5.1 Lemmatization**\n",
    "\n",
    "Lemmatization is the process of converting a word into its base or root form (e.g., \"running\" -> \"run\"). spaCy provides a built-in lemmatizer.\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print(f'{token.text} -> {token.lemma_}')\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SpaCy -> SpaCy\n",
    "is -> be\n",
    "an -> an\n",
    "amazing -> amazing\n",
    "library -> library\n",
    "for -> for\n",
    "NLP -> NLP\n",
    "! -> !\n",
    "```\n",
    "\n",
    "### **5.2 Sentence Segmentation**\n",
    "\n",
    "spaCy automatically segments text into sentences. This can be accessed through the `sents` attribute of the `Doc` object.\n",
    "\n",
    "```python\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "SpaCy is an amazing library for NLP!\n",
    "```\n",
    "\n",
    "### **5.3 Word Vectors and Similarity**\n",
    "\n",
    "spaCy supports word vectors, which are multi-dimensional representations of words in a continuous vector space. It allows you to compare word similarities.\n",
    "\n",
    "```python\n",
    "# Load a larger model for word vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Compare word similarities\n",
    "word1 = nlp(\"dog\")\n",
    "word2 = nlp(\"cat\")\n",
    "\n",
    "print(f\"Similarity: {word1.similarity(word2)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Advanced Features**\n",
    "\n",
    "### **6.1 Text Classification**\n",
    "\n",
    "SpaCy allows you to train text classifiers, such as sentiment analysis, by adding a custom text classification pipeline component. You can add classifiers to the `nlp` pipeline and use labeled data for training.\n",
    "\n",
    "```python\n",
    "from spacy.pipeline.textcat import Config, TextCategorizer\n",
    "\n",
    "# Add text classifier to pipeline\n",
    "text_cat = TextCategorizer(nlp.vocab, config={\"architecture\": \"bow\"})\n",
    "nlp.add_pipe(text_cat)\n",
    "\n",
    "# Train the classifier on labeled data (training code skipped)\n",
    "```\n",
    "\n",
    "### **6.2 Custom Pipelines and Components**\n",
    "\n",
    "You can build custom pipeline components to modify or enhance the text processing flow. For example, adding a custom entity recognizer:\n",
    "\n",
    "```python\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    print(\"Custom component processing the document\")\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(custom_component, last=True)\n",
    "```\n",
    "\n",
    "### **6.3 Custom Tokenization**\n",
    "\n",
    "You can create a custom tokenizer that handles specific tokenization rules.\n",
    "\n",
    "```python\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load the English model\n",
    "nlp = English()\n",
    "\n",
    "# Custom tokenizer\n",
    "custom_tokenizer = Tokenizer(nlp.vocab)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Training Custom Models**\n",
    "\n",
    "spaCy allows you to train custom models, such as NER or text classification models. The training process involves the following steps:\n",
    "\n",
    "1. Preparing labeled data.\n",
    "2. Choosing a model architecture (e.g., CNN, LSTM).\n",
    "3. Defining a training loop with optimization.\n",
    "4. Evaluating the model's performance.\n",
    "\n",
    "spaCy also supports **transfer learning** with pre-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Applications of spaCy**\n",
    "\n",
    "spaCy is highly effective for a wide range of NLP tasks:\n",
    "\n",
    "- **Named Entity Recognition (NER)**: Identify entities such as persons, locations, and dates.\n",
    "- **Part-of-Speech Tagging (POS)**: Classify words by their grammatical roles.\n",
    "- **Dependency Parsing**: Analyze the syntactic structure of sentences.\n",
    "- **Text Classification**: Classify documents into categories (e.g., spam vs. not spam).\n",
    "- **Machine Translation**: Translate text between languages.\n",
    "- **Summarization**: Extract important information from documents.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Conclusion**\n",
    "\n",
    "spaCy is one of the most powerful and efficient NLP libraries available. It is designed for speed, scalability, and ease of use. Whether you're performing simple tasks like tokenization or more advanced tasks like custom model training, spaCy is highly versatile and can be integrated with deep learning frameworks such as PyTorch and TensorFlow.\n",
    "\n",
    "By understanding its features and concepts, you can build powerful natural language processing systems that scale to production environments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
